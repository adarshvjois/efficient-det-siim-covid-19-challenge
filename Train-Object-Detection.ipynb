{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from map_boxes import mean_average_precision_for_boxes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NFL_ROOT_DIR = r\"/home/puttuboo/Datasets/nfl-health-and-safety-helmet-assignment\"\n",
    "TRAINING_DATA = os.path.join(NFL_ROOT_DIR, \"train_labels.csv\")\n",
    "DATA_DIR = os.path.join(NFL_ROOT_DIR, \"train_frames\")\n",
    "\n",
    "train_df = pd.read_csv(TRAINING_DATA)\n",
    "labels = train_df[\"label\"].unique()\n",
    "NAME_TO_LABEL_MAP = {}\n",
    "label_int_id = 0\n",
    "for ind, key in enumerate(labels):\n",
    "    if key[1:] not in NAME_TO_LABEL_MAP:\n",
    "        NAME_TO_LABEL_MAP[key[1:]] = int(key[1:]) + 1\n",
    "FOLDS = 5\n",
    "SIZE = (1024, 1024)\n",
    "NUM_CLASSES = len(labels)\n",
    "BATCHSIZE = 2\n",
    "SEED = 420\n",
    "MODEL_NAME = \"tf_efficientdet_d0\"\n",
    "ORIGINAL_IMAGE_WIDTH = 1280\n",
    "ORIGINAL_IMAGE_HEIGHT = 720\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make results reproducible\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(NAME_TO_LABEL_MAP)\n",
    "\n",
    "\n",
    "def get_int_label(row):\n",
    "    return int(NAME_TO_LABEL_MAP[row[\"label\"][1:]])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df[\"int_label\"] = train_df.apply(get_int_label, axis=1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_img_path(row):\n",
    "    img_id = row[\"video_frame\"]\n",
    "    path = os.path.join(DATA_DIR,img_id+\".jpg\")\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    else:\n",
    "        # raise ValueError(\"Path does not exist {}\".format(path))\n",
    "        return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df[\"path\"] = train_df.apply(get_img_path, axis=1)\n",
    "# train_df = train_df.sample(10000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df = train_df[~train_df[\"path\"].isna()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def all_boxes_for_frame(group):\n",
    "    return group[[\"left\", \"top\", \"width\", \"height\", \"int_label\"]].values.tolist()\n",
    "\n",
    "if not os.path.exists(os.path.join(NFL_ROOT_DIR, \"trainData.csv\")):\n",
    "    print(\"Creating trainData.csv\")\n",
    "\n",
    "    boxes_for_frame = train_df.groupby([\"video_frame\"]).apply(all_boxes_for_frame)\n",
    "    train_samples_df = train_df[[\"video_frame\", \"gameKey\", \"playID\", \"view\", \"path\"]]\n",
    "    train_samples_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    train_samples_df[\"class\"] = train_samples_df[\"gameKey\"].astype('str') + \"-\" + train_samples_df[\"playID\"].astype('str')\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    train_df[\"fold\"] = -1\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        skf.split(X=train_samples_df[\"video_frame\"], y=train_samples_df[\"class\"])\n",
    "    ):\n",
    "        train_samples_df.loc[train_samples_df.iloc[val_idx].index, \"fold\"] = fold\n",
    "    \n",
    "    train_samples_df.set_index(\"video_frame\", inplace=True)\n",
    "    train_samples_df[\"boxes\"] = boxes_for_frame\n",
    "    train_samples_df.to_csv(os.path.join(NFL_ROOT_DIR, \"trainData.csv\"))\n",
    "    del train_samples_df\n",
    "\n",
    "train_samples_df = pd.read_csv(os.path.join(NFL_ROOT_DIR, \"trainData.csv\"))\n",
    "train_samples_df.set_index(\"video_frame\", inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# dev = torch.device(\"cpu\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class NFLDatasetFromDFHelmetAnnotations(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        train=True,\n",
    "        predict=True,\n",
    "        augment=True,\n",
    "        data_dir=NFL_ROOT_DIR,\n",
    "        size=SIZE,\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.label_list = NAME_TO_LABEL_MAP\n",
    "        self.ids = df.index.sort_values()  # [:100]\n",
    "        self.path_suffix = data_dir\n",
    "        self._augment = augment\n",
    "        self._train = train\n",
    "        self._predict = predict\n",
    "        self._size = size\n",
    "        self._transform_list = [\n",
    "            # A.Resize(size[0], size[1], p=1)\n",
    "        ]\n",
    "\n",
    "        if self._augment:\n",
    "            self._transform_list.extend(\n",
    "                [\n",
    "                    A.VerticalFlip(p=0.5),\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    A.ShiftScaleRotate(\n",
    "                        scale_limit=0.20,\n",
    "                        rotate_limit=10,\n",
    "                        shift_limit=0.1,\n",
    "                        p=0.5,\n",
    "                        border_mode=cv2.BORDER_CONSTANT,\n",
    "                        value=0,\n",
    "                    ),\n",
    "                    A.RandomBrightnessContrast(p=0.5),\n",
    "                    # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                    # ToTensorV2(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        if self._train or self._predict:\n",
    "            self._transform_list.extend(\n",
    "                [\n",
    "                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                    ToTensorV2(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        if self._transform_list:\n",
    "\n",
    "            self._transforms = A.Compose(\n",
    "                self._transform_list,\n",
    "                bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"labels\"]),\n",
    "            )\n",
    "        self._preprocess_boxes()\n",
    "\n",
    "    def _preprocess_boxes(self):\n",
    "        self.id_bbox_map = {}\n",
    "        scaled_w = self._size[1]\n",
    "        scaled_h = self._size[0]\n",
    "        opacity_count = 0\n",
    "        none_count = 0\n",
    "        for i, id in enumerate(self.ids):\n",
    "            row = self.df.loc[id]\n",
    "            all_boxes = []\n",
    "            all_cats = []\n",
    "            boxes = eval(row[\"boxes\"])\n",
    "            for annotation in boxes:\n",
    "                box = {}\n",
    "                # left,top,width,height\n",
    "                # convert to center and normalize to 0, 1\n",
    "                box[\"x\"] = (\n",
    "                    (max(0, int(annotation[0]))) * scaled_w / (ORIGINAL_IMAGE_WIDTH)\n",
    "                )\n",
    "                box[\"y\"] = (\n",
    "                    (max(0, int(annotation[1]))) * scaled_h / (ORIGINAL_IMAGE_HEIGHT)\n",
    "                )\n",
    "                box[\"width\"] = int(annotation[2]) * scaled_w / ORIGINAL_IMAGE_WIDTH\n",
    "                box[\"height\"] = int(annotation[3]) * scaled_h / ORIGINAL_IMAGE_HEIGHT\n",
    "                bbox = [\n",
    "                    box[\"x\"],\n",
    "                    box[\"y\"],\n",
    "                    box[\"x\"] + (box[\"width\"]),\n",
    "                    box[\"y\"] + (box[\"height\"]),\n",
    "                ]\n",
    "\n",
    "                all_boxes.append(bbox)\n",
    "                all_cats.append(annotation[-1])\n",
    "            self.id_bbox_map[id] = (all_boxes, all_cats)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def draw_bbox_idx(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        row = self.df.loc[img_id]\n",
    "        print(img_id)\n",
    "        image = PIL.Image.open(row[\"path\"])\n",
    "        scaled_w = image.width\n",
    "        scaled_h = image.height\n",
    "        print((scaled_w, scaled_h))\n",
    "        boxes = eval(row[\"boxes\"])\n",
    "        draw = PIL.ImageDraw.Draw(image)\n",
    "        for annotation in boxes:\n",
    "            box = {}\n",
    "            box[\"x\"] = int(annotation[0]) / 1280\n",
    "            box[\"y\"] = int(annotation[1]) / 720\n",
    "            box[\"width\"] = int(annotation[2]) / 1280\n",
    "            box[\"height\"] = int(annotation[3]) / 720\n",
    "            draw.rectangle(\n",
    "                [\n",
    "                    box[\"x\"] * scaled_w,\n",
    "                    box[\"y\"] * scaled_h,\n",
    "                    (box[\"x\"] + box[\"width\"]) * scaled_w,\n",
    "                    (box[\"y\"] + box[\"height\"]) * scaled_h,\n",
    "                ]\n",
    "            )\n",
    "        return image\n",
    "\n",
    "    def _yolo_to_voc_format(self, yolo_bboxes):\n",
    "        # takes in bounding boxes of the yolo format\n",
    "        # converts them to voc format.\n",
    "        # yolo format (x_c, y_c, width, height) normalized to 0, 1 by dividing by image dims.\n",
    "        # voc format (x_min, y_min, x_max, y_max), unnormalized.\n",
    "        scaled_w = self._size[1]\n",
    "        scaled_h = self._size[0]\n",
    "        bboxes_voc = torch.zeros_like(yolo_bboxes)\n",
    "        # x_min = (x_c - width / 2) * scaled_w\n",
    "        bboxes_voc[:, 0] = (yolo_bboxes[:, 0] - yolo_bboxes[:, 2] / 2) * scaled_w\n",
    "        bboxes_voc[:, 1] = (yolo_bboxes[:, 1] - yolo_bboxes[:, 3] / 2) * scaled_h\n",
    "        bboxes_voc[:, 2] = bboxes_voc[:, 0] + yolo_bboxes[:, 2] * scaled_w\n",
    "        bboxes_voc[:, 3] = bboxes_voc[:, 1] + yolo_bboxes[:, 3] * scaled_h\n",
    "\n",
    "        return bboxes_voc\n",
    "\n",
    "    def draw_bbox_img(self, image, bboxes, label):\n",
    "        image = PIL.Image.fromarray(image)\n",
    "        draw = PIL.ImageDraw.Draw(image)\n",
    "        for bbox in bboxes:\n",
    "            # x_c = bbox[0]\n",
    "            # y_c = bbox[1]\n",
    "            # width = bbox[2]\n",
    "            # height = bbox[3]\n",
    "            # x_1 = (x_c - width / 2) * image.width\n",
    "            # y_1 = (y_c - height / 2) * image.height\n",
    "            # x_2 = x_1 + width * image.width\n",
    "            # y_2 = y_1 + height * image.height\n",
    "            # draw.rectangle([x_1, y_1, x_2, y_2])\n",
    "            draw.rectangle([bbox[0], bbox[1], bbox[2], bbox[3]])\n",
    "        print(f\"Number of boxes{len(label)}\")\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        boxes, labels = self.id_bbox_map[img_id]\n",
    "        row = self.df.loc[img_id]\n",
    "\n",
    "        path = row[\"path\"]\n",
    "        # ideally, we'd clean up the df,\n",
    "        # but may be we use it to produce predictions as well.\n",
    "        dicom_arr = cv2.imread(path)\n",
    "        img = cv2.cvtColor(dicom_arr, cv2.COLOR_BGR2RGB)\n",
    "        image_and_labels = {}\n",
    "        if self._augment or (self._train or self._predict):\n",
    "            image_and_labels = self._transforms(image=img, bboxes=boxes, labels=labels)\n",
    "        else:\n",
    "            image_and_labels = {\"image\": img, \"bboxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        # image_and_labels[\"bboxes\"] = self._yolo_to_voc_format(\n",
    "        #     torch.tensor(image_and_labels[\"bboxes\"])\n",
    "        # )\n",
    "\n",
    "        # if not image_and_labels[\"bboxes\"]:\n",
    "        #     w = torch.rand(1) * 10\n",
    "        #     h = torch.rand(1) * 10\n",
    "        #     image_and_labels[\"bboxes\"] = [[self._size[0] /2 - w, self._size[1]/2 - h, self._size[0]/2 + w, self._size[1]/2 + h]]\n",
    "        #     image_and_labels[\"labels\"] = [-1]\n",
    "        # print(image_and_labels[\"bboxes\"].shape)\n",
    "        image_and_labels[\"bboxes\"] = torch.tensor(image_and_labels[\"bboxes\"]).to(\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        image_and_labels[\"labels\"] = torch.tensor(image_and_labels[\"labels\"]).to(\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        return image_and_labels\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_ds = NFLDatasetFromDFHelmetAnnotations(\n",
    "    train_samples_df, train=False, predict=False, augment=True, size=SIZE\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = training_ds[100]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[\"bboxes\"].dtype"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_ds.draw_bbox_idx(100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_epoch(\n",
    "    epoch,\n",
    "    step,\n",
    "    dataloader,\n",
    "    batchsize,\n",
    "    model_w_loss,\n",
    "    optimizer,\n",
    "    steps_per_epoch,\n",
    "    log_every=10,\n",
    "    scaler=None,\n",
    "):\n",
    "\n",
    "    steps = steps_per_epoch\n",
    "    batchsize = batchsize\n",
    "    dataiter = iter(dataloader)\n",
    "\n",
    "    time_now = time.time()\n",
    "\n",
    "    total_loss_avg = AverageMeter()\n",
    "    cls_loss_avg = AverageMeter()\n",
    "    reg_loss_avg = AverageMeter()\n",
    "\n",
    "    model_w_loss.train()\n",
    "    loader = tqdm(range(steps))\n",
    "\n",
    "    enable_autocast = not (scaler == None)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        data = next(dataiter)\n",
    "        images, bboxes, labels = data[\"image\"], data[\"bboxes\"], data[\"labels\"]\n",
    "        images = images.to(dev)\n",
    "        bboxes = [bbox.float().to(dev) for bbox in bboxes]\n",
    "        labels = [label.to(dev) for label in labels]\n",
    "\n",
    "        with amp.autocast(enabled=enable_autocast):\n",
    "            # hacky timm target assembly\n",
    "            target = {\"bbox\": bboxes, \"cls\": labels}\n",
    "            losses = model_w_loss(images, target)\n",
    "            cls_loss, reg_loss = losses[\"class_loss\"], losses[\"box_loss\"]\n",
    "            # print(\"cls loss is na {}\".format(str(cls_loss.isnan().any())))\n",
    "\n",
    "            # cls_loss = cls_loss.mean()\n",
    "            # reg_loss = reg_loss.mean()\n",
    "            loss = losses[\"loss\"]\n",
    "\n",
    "        if enable_autocast:\n",
    "            scaled_loss = scaler.scale(loss)\n",
    "            scaled_loss.backward()\n",
    "        else:\n",
    "            total_loss_avg.update(loss.item(), batchsize)\n",
    "            loss.backward()\n",
    "        total_loss_avg.update(loss.item(), batchsize)\n",
    "        cls_loss_avg.update(cls_loss.item(), batchsize)\n",
    "        reg_loss_avg.update(reg_loss.item(), batchsize)\n",
    "\n",
    "        if scaler:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        time_spent = time.time() - time_now\n",
    "        time_now = time.time()\n",
    "\n",
    "        loader.set_description(\n",
    "            \"Training Epoch : {}, Time Spent {:.5g}, Step {}\".format(\n",
    "                epoch, time_spent, step\n",
    "            )\n",
    "        )\n",
    "        loader.set_postfix(\n",
    "            loss=total_loss_avg.avg,\n",
    "            reg_loss=reg_loss_avg.avg,\n",
    "            cls_loss=cls_loss_avg.avg,\n",
    "        )\n",
    "        loader.update()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return cls_loss_avg.avg, reg_loss_avg.avg, total_loss_avg.avg, step\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def normalize_boxes_in_df(boxes_df):\n",
    "    boxes_df[\"XMin\"] = boxes_df[\"XMin\"] / SIZE[0]\n",
    "    boxes_df[\"YMin\"] = boxes_df[\"YMin\"] / SIZE[1]\n",
    "    boxes_df[\"XMax\"] = boxes_df[\"XMax\"] / SIZE[0]\n",
    "    boxes_df[\"YMax\"] = boxes_df[\"YMax\"] / SIZE[1]\n",
    "    return boxes_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def valid_epoch(epoch, step, dataloader, batchsize, model_w_loss, gt_df):\n",
    "    num_val = len(dataloader)\n",
    "\n",
    "    dataiter = iter(dataloader)\n",
    "    model_w_loss.eval()\n",
    "\n",
    "    detections = []\n",
    "\n",
    "    total_loss_avg = AverageMeter()\n",
    "    cls_loss_avg = AverageMeter()\n",
    "    reg_loss_avg = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_val)):\n",
    "            data = next(dataiter)\n",
    "            images, bboxes, labels = data[\"image\"], data[\"bboxes\"], data[\"labels\"]\n",
    "            batchsize = images.shape[0]\n",
    "\n",
    "            images = images.to(dev)\n",
    "\n",
    "            bboxes = [bbox.float().to(dev) for bbox in bboxes]\n",
    "            labels = [label.to(dev) for label in labels]\n",
    "            target = {\n",
    "                \"bbox\": bboxes,\n",
    "                \"cls\": labels,\n",
    "                \"img_scale\": torch.tensor(\n",
    "                    [1.0] * images.shape[0], dtype=torch.float32\n",
    "                ).to(dev),\n",
    "                \"img_size\": torch.tensor(\n",
    "                    [images.shape[-2:]] * images.shape[0], dtype=torch.float32\n",
    "                ).to(dev),\n",
    "            }\n",
    "            losses_w_detections = model_w_loss(images, target)\n",
    "            cls_loss, reg_loss = (\n",
    "                losses_w_detections[\"class_loss\"],\n",
    "                losses_w_detections[\"box_loss\"],\n",
    "            )\n",
    "            loss = losses_w_detections[\"loss\"]\n",
    "            total_loss_avg.update(loss.item(), batchsize)\n",
    "            cls_loss_avg.update(cls_loss.item(), batchsize)\n",
    "            reg_loss_avg.update(reg_loss.item(), batchsize)\n",
    "\n",
    "            batched_detections = losses_w_detections[\"detections\"]\n",
    "            detections.append(batched_detections)\n",
    "\n",
    "        offset = 0\n",
    "        ids = dataloader.dataset.ids\n",
    "        label_list = dataloader.dataset.label_list\n",
    "        all_preds = []\n",
    "        for batched_detections in detections:\n",
    "            num_imgs, num_dets_per_img = (\n",
    "                batched_detections.shape[0],\n",
    "                batched_detections.shape[1],\n",
    "            )\n",
    "            img_ids = []\n",
    "\n",
    "            for i in range(num_imgs):\n",
    "                img_ids.extend([ids[offset]] * num_dets_per_img)\n",
    "                offset += 1\n",
    "\n",
    "            batched_preds_df = pd.DataFrame.from_records(\n",
    "                batched_detections.view(num_imgs * num_dets_per_img, -1).tolist(),\n",
    "                columns=[\"XMin\", \"YMin\", \"XMax\", \"YMax\", \"Conf\", \"LabelName\"],\n",
    "            )\n",
    "            batched_preds_df[\"ImageID\"] = img_ids\n",
    "            batched_preds_df[\"LabelName\"] = batched_preds_df.apply(\n",
    "                lambda x: str(int(x[\"LabelName\"])), axis=1\n",
    "            )\n",
    "            all_preds.append(batched_preds_df)\n",
    "        all_preds_df = pd.concat(all_preds)\n",
    "        print(all_preds_df.columns)\n",
    "        print(gt_df.columns)\n",
    "\n",
    "        # gt_df = normalize_boxes_in_df(gt_df)\n",
    "        # all_preds_df = normalize_boxes_in_df(all_preds_df)\n",
    "        print(gt_df[\"LabelName\"].unique())\n",
    "        print(all_preds_df[\"LabelName\"].unique())\n",
    "\n",
    "        mean_ap, average_precisions = mean_average_precision_for_boxes(\n",
    "            gt_df[[\"ImageID\", \"LabelName\", \"XMin\", \"XMax\", \"YMin\", \"YMax\"]],\n",
    "            all_preds_df[\n",
    "                [\"ImageID\", \"LabelName\", \"Conf\", \"XMin\", \"XMax\", \"YMin\", \"YMax\"]\n",
    "            ],\n",
    "            iou_threshold=0.5,\n",
    "            verbose=True,\n",
    "        )\n",
    "    # topk = (-probs).argsort(axis=1)[:, :2]\n",
    "    # # mapk = mean_average_precision(targets[:, np.newaxis].tolist(), topk.tolist(), k=2)\n",
    "    print(\"Mean AP @0.5 Score at Epoch {} and Step {}: {}\".format(epoch, step, mean_ap))\n",
    "    for k in average_precisions:\n",
    "        print(\n",
    "            \"AP @0.5 Score for Opacity at Epoch {} and Step {}: {}\".format(\n",
    "                epoch, step, average_precisions[k]\n",
    "            )\n",
    "        )\n",
    "    # print(\"AUC Score at Epoch {} and Step {}: {}\".format(epoch, step, auc))\n",
    "    return (\n",
    "        mean_ap,\n",
    "        total_loss_avg.avg,\n",
    "        cls_loss_avg.avg,\n",
    "        reg_loss_avg.avg,\n",
    "        all_preds_df,\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_model(\n",
    "    model_w_loss,\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    batchsize,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    save_path,\n",
    "    train_dl,\n",
    "    validation_dl,\n",
    "    val_gt_df,\n",
    "    use_mp=True,\n",
    "):\n",
    "    if use_mp:\n",
    "        scaler = amp.GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    if not steps_per_epoch:\n",
    "            steps_per_epoch = len(training_dl)\n",
    "    train_vs_val = []\n",
    "\n",
    "    reg_loss_best = float(\"inf\")\n",
    "    mean_ap_best = -float(\"inf\")\n",
    "    ap_opacity_best = -float(\"inf\")\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_cls_loss, train_reg_loss, train_total_loss, step = train_epoch(\n",
    "            epoch, step,train_dl, batchsize, model_w_loss, optimizer, steps_per_epoch, scaler=scaler\n",
    "        )\n",
    "\n",
    "        time_now = time.time()\n",
    "        (\n",
    "            mean_ap,\n",
    "            val_total_loss,\n",
    "            val_cls_loss,\n",
    "            val_reg_loss,\n",
    "            preds_df,\n",
    "        ) = valid_epoch(epoch, step, validation_dl, batchsize, model_w_loss, val_gt_df)\n",
    "        # val_mapk, val_auc, val_loss_avg, probs, targets = valid_epoch(\n",
    "        #     epoch, step, validation_dl, batchsize * 2, model, loss_fn\n",
    "        # )\n",
    "\n",
    "        train_vs_val.extend(\n",
    "            [\n",
    "                (epoch, mean_ap, \"Mean AP\"),\n",
    "                (epoch, val_total_loss, \"Validation Total Loss\"),\n",
    "                (epoch, val_reg_loss, \"Train Regression Loss\"),\n",
    "                (epoch, val_cls_loss, \"Train Classification Loss\"),\n",
    "                (epoch, train_total_loss, \"Training Total Loss\"),\n",
    "                (epoch, train_reg_loss, \"Train Regression Loss\"),\n",
    "                (epoch, train_cls_loss, \"Train Classification Loss\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_total_loss)\n",
    "            print(\n",
    "                \"Setting Learning Rate to: {:.6f}\".format(\n",
    "                    optimizer.param_groups[-1][\"lr\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if reg_loss_best > val_reg_loss:\n",
    "            print(\n",
    "                \"Found Model with best Regression Loss {} at epoch {}\".format(\n",
    "                    val_reg_loss, epoch\n",
    "                )\n",
    "            )\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"mAP\": mean_ap,\n",
    "                    \"TotalLoss\": val_total_loss,\n",
    "                    \"state_dict\": model_w_loss.module.model.state_dict(),\n",
    "                },\n",
    "                os.path.join(save_path, \"best_reg_loss.pth\"),\n",
    "            )\n",
    "            preds_df.to_csv(\n",
    "                os.path.join(save_path, \"best_reg_loss_preds.csv\"), index=False\n",
    "            )\n",
    "            reg_loss_best = val_reg_loss\n",
    "\n",
    "        if reg_loss_best > val_total_loss:\n",
    "            print(\n",
    "                \"Found Model with best Total Loss {} at epoch {}\".format(\n",
    "                    val_total_loss, epoch\n",
    "                )\n",
    "            )\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"mAP\": mean_ap,\n",
    "                    \"TotalLoss\": val_total_loss,\n",
    "                    \"state_dict\": model_w_loss.module.model.state_dict(),\n",
    "                },\n",
    "                os.path.join(save_path, \"best_total_loss.pth\"),\n",
    "            )\n",
    "            preds_df.to_csv(\n",
    "                os.path.join(save_path, \"best_total_loss_preds.csv\"), index=False\n",
    "            )\n",
    "            reg_loss_best = val_total_loss\n",
    "\n",
    "        if mean_ap_best < mean_ap:\n",
    "            print(\"Found Model with best Mean AP {} at epoch {}\".format(mean_ap, epoch))\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"mAP\": mean_ap,\n",
    "                    \"TotalLoss\": val_total_loss,\n",
    "                    \"state_dict\": model_w_loss.module.model.state_dict(),\n",
    "                },\n",
    "                os.path.join(save_path, \"best_mean_ap.pth\"),\n",
    "            )\n",
    "            mean_ap_best = mean_ap\n",
    "            preds_df.to_csv(\n",
    "                os.path.join(save_path, \"best_mean_ap_preds.csv\"), index=False\n",
    "            )\n",
    "\n",
    "        time_spent = time.time() - time_now\n",
    "        print(\n",
    "            \"{}, Epoch : {}, Step : {}, Validation Loss : {:.5f}, Mean AP : {:.5f}, Run Time : {:.5g}\".format(\n",
    "                time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                epoch,\n",
    "                step,\n",
    "                val_total_loss,\n",
    "                mean_ap,\n",
    "                time_spent,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return train_vs_val\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def collater(data):\n",
    "    imgs = [s[\"image\"] for s in data]\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    for i, s in enumerate(data):\n",
    "        # if len(s[\"bboxes\"].shape) != 2:\n",
    "        #     print(\"came here\")\n",
    "        #     width = torch.rand(1) * 10\n",
    "        #     height = torch.rand(1) * 10\n",
    "        #     bboxes.append(torch.rand((4)) * 10)\n",
    "        #     labels.append(torch.tensor([[1]]))\n",
    "        # else:\n",
    "            # rearrange to timm format\n",
    "        bboxes.append(s[\"bboxes\"][:, [1, 0, 3, 2]])\n",
    "        labels.append(s[\"labels\"])\n",
    "    # labels = [s[\"labels\"] for s in data]\n",
    "\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "\n",
    "    max_boxes = max(bbox.shape[0] for bbox in bboxes)\n",
    "\n",
    "    # if max_boxes > 0:\n",
    "\n",
    "    #     annot_padded = torch.ones((len(bboxes), max_boxes, 4)) * -1\n",
    "\n",
    "    #     for idx, annot in enumerate(bboxes):\n",
    "    #         if annot.shape[0] > 0:\n",
    "    #             annot_padded[idx, :annot.shape[0], :-1] = annot\n",
    "    #             annot_padded[idx, :annot.shape[0], -1] = labels[idx]\n",
    "    # else:\n",
    "    #     annot_padded = torch.ones((len(bboxes), 1, 5)) * -1\n",
    "\n",
    "    # imgs = imgs.permute(0, 3, 1, 2)\n",
    "\n",
    "    return {\"image\": imgs, \"bboxes\": bboxes, \"labels\": labels}\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_gt_df_from_ds(dataset):\n",
    "    label_list = dataset.label_list\n",
    "    gt_df = []\n",
    "    for i, data in enumerate(dataset):\n",
    "        bboxes = data[\"bboxes\"].tolist()\n",
    "        labels = data[\"labels\"].tolist()\n",
    "        id = dataset.ids[i]\n",
    "        for j, box in enumerate(bboxes):\n",
    "            box.extend([labels[j], id])\n",
    "        \n",
    "        img_df = pd.DataFrame.from_records(\n",
    "            bboxes, columns=[\"XMin\", \"YMin\", \"XMax\", \"YMax\", \"LabelName\", \"ImageID\"]\n",
    "        )\n",
    "        gt_df.append(img_df)\n",
    "    print(pd.concat(gt_df).columns)\n",
    "    gt_df = pd.concat(gt_df)\n",
    "    gt_df[\"LabelName\"] = gt_df.apply(lambda x: str(int(x[\"LabelName\"])), axis=1)\n",
    "    return gt_df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_net():\n",
    "    config = get_efficientdet_config(MODEL_NAME)\n",
    "\n",
    "    config.image_size = SIZE\n",
    "    config.norm_kwargs = dict(eps=0.001, momentum=0.01)\n",
    "\n",
    "    net = EfficientDet(config, pretrained_backbone=True)\n",
    "    num_classes =  len(set(NAME_TO_LABEL_MAP.values()))\n",
    "    print(\"Num Classes : {}\".format(num_classes))\n",
    "    net.reset_head(num_classes= num_classes)\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "    print(config)\n",
    "\n",
    "    return DetBenchTrain(net, config)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for fold in range(FOLDS):\n",
    "    print(\"Training Fold {}\".format(fold))\n",
    "    model = get_net()\n",
    "\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model = model.to(dev)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.1, patience=5, verbose=False, min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    training_fold = train_samples_df[train_samples_df[\"fold\"] != fold]\n",
    "    validation_fold = train_samples_df[train_samples_df[\"fold\"] == fold]\n",
    "\n",
    "    validation_fold.to_csv(\"validation_fold-{}.csv\".format(fold))\n",
    "    training_fold.to_csv(\"training_fold-{}.csv\".format(fold))\n",
    "    training_ds = NFLDatasetFromDFHelmetAnnotations(\n",
    "        training_fold,#.sample(5000),\n",
    "        augment=True,\n",
    "        train=True,\n",
    "        size=SIZE,\n",
    "    )\n",
    "    validation_ds = NFLDatasetFromDFHelmetAnnotations(\n",
    "        validation_fold,#.sample(500),\n",
    "        predict=True,\n",
    "        augment=False,\n",
    "        size=SIZE,\n",
    "    )\n",
    "\n",
    "    val_df = make_gt_df_from_ds(validation_ds)\n",
    "    print(\"{} Ground Truth size\".format(str(val_df.shape)))\n",
    "    print(\"{} train len {} val len\".format(len(training_ds), len(validation_ds)))\n",
    "\n",
    "    training_dl = torch.utils.data.DataLoader(\n",
    "        dataset=training_ds,\n",
    "        batch_size=BATCHSIZE,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        collate_fn=collater,\n",
    "        # prefetch_factor=8,\n",
    "    )\n",
    "    validation_dl = torch.utils.data.DataLoader(\n",
    "        dataset=validation_ds,\n",
    "        batch_size=BATCHSIZE,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        # prefetch_factor=8,\n",
    "        collate_fn=collater,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"{} training data loader size {} validation dataloader size\".format(\n",
    "            len(training_dl), len(validation_dl)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    train_vs_val = train_model(\n",
    "        model_w_loss=model,\n",
    "        steps_per_epoch=4000,\n",
    "        epochs=20,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        batchsize=BATCHSIZE,\n",
    "        save_path=\"{}-{}\".format(MODEL_NAME, fold),\n",
    "        train_dl=training_dl,\n",
    "        validation_dl=validation_dl,\n",
    "        val_gt_df=val_df,\n",
    "    )\n",
    "\n",
    "    fold_report = pd.DataFrame.from_records(\n",
    "        data=train_vs_val, columns=[\"Epoch\", \"Loss\", \"Type\"]\n",
    "    )\n",
    "    fold_report.to_csv(\"fold-{}-report.csv\".format(fold), index=False)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8395b64272c073928528f930764fd8b6be4f406ea8255e5556570dd7fe787a11"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}