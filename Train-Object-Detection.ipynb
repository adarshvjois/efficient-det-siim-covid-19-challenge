{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import glob\r\n",
    "import os\r\n",
    "import time\r\n",
    "import random\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import PIL\r\n",
    "\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "\r\n",
    "import cv2\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "from tqdm.contrib.concurrent import process_map\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch.utils.data.dataset import Dataset\r\n",
    "import torch.cuda.amp as amp\r\n",
    "import pydicom\r\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\r\n",
    "\r\n",
    "import albumentations as A\r\n",
    "from albumentations.pytorch import ToTensorV2\r\n",
    "\r\n",
    "from map_boxes import mean_average_precision_for_boxes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\r\n",
    "from effdet.efficientdet import HeadNet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATA_DIR = \"../\"\r\n",
    "RESIZE_DIR = \"../\"\r\n",
    "\r\n",
    "SIZE = (512, 512)\r\n",
    "FOLDS = 5\r\n",
    "NUM_CLASSES = 4\r\n",
    "BATCHSIZE = 16\r\n",
    "SEED = 420\r\n",
    "MODEL_NAME = \"tf_efficientdet_d1\"\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make results reproducible\r\n",
    "def seed_everything(seed):\r\n",
    "    random.seed(seed)\r\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\r\n",
    "    np.random.seed(seed)\r\n",
    "    torch.manual_seed(seed)\r\n",
    "    torch.cuda.manual_seed(seed)\r\n",
    "    torch.backends.cudnn.benchmark = False\r\n",
    "    torch.backends.cudnn.deterministic = True\r\n",
    "\r\n",
    "\r\n",
    "seed_everything(SEED)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_images_df = pd.read_csv(os.path.join(DATA_DIR, \"train_image_level.csv\"))\r\n",
    "train_study_df = pd.read_csv(os.path.join(DATA_DIR, \"train_study_level.csv\"))\r\n",
    "train_images_df[\"StudyInstanceUID\"] = train_images_df[\"StudyInstanceUID\"] + \"_study\"\r\n",
    "\r\n",
    "train_study_df.columns = train_study_df.columns.map(lambda x: x.split(\" \")[0])\r\n",
    "\r\n",
    "train_study_df.rename(columns={\"id\": \"study_id\"}, inplace=True)\r\n",
    "train_images_df.rename(columns={\"StudyInstanceUID\": \"study_id\"}, inplace=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_meta_df = pd.read_csv(os.path.join(DATA_DIR, \"train_meta.csv\"))\r\n",
    "train_meta_df.rename(columns={\"fname\": \"id\"}, inplace=True)\r\n",
    "train_meta_df[\"id\"] = train_meta_df[\"id\"] + \"_image\"\r\n",
    "train_images_df = pd.merge(train_images_df, train_meta_df, how=\"inner\", on=\"id\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if NUM_CLASSES == 4:\r\n",
    "    NAME_TO_LABEL_MAP = {\"Negative\": 0, \"Typical\": 1, \"Indeterminate\": 2, \"Atypical\": 3}\r\n",
    "\r\n",
    "\r\n",
    "def get_str_label(row):\r\n",
    "    for k in NAME_TO_LABEL_MAP:\r\n",
    "        if row[k]:\r\n",
    "            return k\r\n",
    "    return None\r\n",
    "\r\n",
    "\r\n",
    "def get_int_label(row):\r\n",
    "    for k in NAME_TO_LABEL_MAP:\r\n",
    "        if row[k]:\r\n",
    "            return NAME_TO_LABEL_MAP[k]\r\n",
    "    return None\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_study_df[\"int_label\"] = train_study_df.apply(get_int_label, axis=1)\r\n",
    "train_study_df[\"str_label\"] = train_study_df.apply(get_str_label, axis=1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_samples_df = pd.merge(\r\n",
    "    train_images_df, train_study_df, on=\"study_id\", how=\"inner\"\r\n",
    ").reset_index(drop=True)\r\n",
    "\r\n",
    "box_and_images_counts_df = (\r\n",
    "    train_samples_df.groupby(\"study_id\")[[\"id\", \"boxes\"]]\r\n",
    "    .count()\r\n",
    "    .sort_values(ascending=False, by=\"id\")\r\n",
    "    .reset_index()\r\n",
    ")\r\n",
    "box_and_images_counts_df.rename(\r\n",
    "    columns={\"id\": \"id_count\", \"boxes\": \"boxes_count\"}, inplace=True\r\n",
    ")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_samples_df = pd.merge(\r\n",
    "    train_samples_df, box_and_images_counts_df, how=\"inner\", on=\"study_id\"\r\n",
    ")\r\n",
    "train_samples_df.sort_values([\"id_count\", \"boxes_count\"], ascending=False, inplace=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def resize(array, size, keep_ratio=False, resample=PIL.Image.LANCZOS):\r\n",
    "    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\r\n",
    "    im = PIL.Image.fromarray(array)\r\n",
    "\r\n",
    "    if keep_ratio:\r\n",
    "        im.thumbnail((size[0], size[1]), resample)\r\n",
    "    else:\r\n",
    "        im = im.resize((size[0], size[1]), resample)\r\n",
    "\r\n",
    "    return im\r\n",
    "\r\n",
    "\r\n",
    "def dicom2array(path, size, voi_lut=True, fix_monochrome=True):\r\n",
    "    dicom = pydicom.read_file(path)\r\n",
    "    # VOI LUT (if available by DICOM device) is used to\r\n",
    "    # transform raw DICOM data to \"human-friendly\" view\r\n",
    "    if voi_lut:\r\n",
    "        data = apply_voi_lut(dicom.pixel_array, dicom)\r\n",
    "    else:\r\n",
    "        data = dicom.pixel_array\r\n",
    "    # depending on this value, X-ray may look inverted - fix that:\r\n",
    "    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\r\n",
    "        data = np.amax(data) - data\r\n",
    "    data = data - np.min(data)\r\n",
    "    data = data / np.max(data)\r\n",
    "    data = (data * 255).astype(np.uint8)\r\n",
    "    data = resize(data, size)\r\n",
    "    return data\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "TRAIN_DATA_PATH = None\r\n",
    "\r\n",
    "if os.path.exists(os.path.join(RESIZE_DIR, 'train_{}x{}'.format(SIZE[0], SIZE[1]))):\r\n",
    "    TRAIN_DATA_PATH = os.path.join(RESIZE_DIR, 'train_{}x{}'.format(SIZE[0], SIZE[1]))\r\n",
    "    print(\"{} Exists\".format(TRAIN_DATA_PATH))\r\n",
    "else:\r\n",
    "    TRAIN_DATA_PATH = os.path.join(RESIZE_DIR, 'train_{}x{}'.format(SIZE[0], SIZE[1]))\r\n",
    "    print(\"Creating Training dir at {}\".format(TRAIN_DATA_PATH))\r\n",
    "    os.makedirs(TRAIN_DATA_PATH)\r\n",
    "    filenames = glob.glob(os.path.join(DATA_DIR, \"train/*/*/*.dcm\"))\r\n",
    "\r\n",
    "    def persist_image(path):\r\n",
    "        im = dicom2array(path, SIZE)\r\n",
    "        fname = os.path.basename(os.path.splitext(path)[-2])\r\n",
    "        jpg_fname = os.path.join(TRAIN_DATA_PATH, \"{}.jpg\".format(fname))\r\n",
    "        im.save(jpg_fname)\r\n",
    "        return jpg_fname\r\n",
    "\r\n",
    "    process_map(persist_image, filenames, max_workers=8, chunksize=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def keep_row(row):\r\n",
    "    # keep as negative sample for study with 0 bboxes with opacity\r\n",
    "    # or non-null bounding box\r\n",
    "    if row[\"boxes_count\"] and not pd.isna(row[\"boxes\"]):\r\n",
    "        return True\r\n",
    "    if row[\"boxes_count\"] == 0:\r\n",
    "        return True\r\n",
    "#     if pd.isna(row[\"boxes\"]):\r\n",
    "#         return False\r\n",
    "    else:\r\n",
    "        return True\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_img_path(row):\r\n",
    "    img_id = row[\"id\"][:-6]\r\n",
    "    paths = glob.glob(os.path.join(TRAIN_DATA_PATH, \"{}.jpg\".format(img_id)))\r\n",
    "    for path in paths:\r\n",
    "        if img_id in path:\r\n",
    "            return path\r\n",
    "    return None\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_samples_df[\"keep_row\"] = train_samples_df.apply(keep_row, axis=1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_samples_df = train_samples_df[train_samples_df[\"keep_row\"]]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_samples_df[\"path\"] = train_samples_df.apply(get_img_path, axis=1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\r\n",
    "train_samples_df[\"fold\"] = -1\r\n",
    "for fold, (train_idx, val_idx) in enumerate(\r\n",
    "    skf.split(X=train_samples_df[\"id\"], y=train_samples_df[\"int_label\"].tolist())\r\n",
    "):\r\n",
    "    train_samples_df.loc[train_samples_df.iloc[val_idx].index, \"fold\"] = fold\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_samples_df.sort_values([\"id\"], inplace=True)\r\n",
    "train_samples_df.set_index(\"id\", inplace=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.catplot(x=\"str_label\", col=\"fold\", data=train_samples_df, kind=\"count\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\r\n",
    "dev\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class XRayDatasetFromDFOpacityAnnotations(Dataset):\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        df,\r\n",
    "        train=True,\r\n",
    "        predict=True,\r\n",
    "        augment=True,\r\n",
    "        data_dir=os.path.join(DATA_DIR, \"train\"),\r\n",
    "        size=(384, 384),\r\n",
    "    ):\r\n",
    "        self.df = df\r\n",
    "        self.label_list = [\"none\", \"opacity\"]\r\n",
    "        self.ids = df.index.sort_values()#[:100]\r\n",
    "        self.path_suffix = data_dir\r\n",
    "        self._augment = augment\r\n",
    "        self._train = train\r\n",
    "        self._predict = predict\r\n",
    "        self._size = size\r\n",
    "        self._transform_list = [\r\n",
    "            # A.Resize(size[0], size[1], p=1)\r\n",
    "        ]\r\n",
    "\r\n",
    "        if self._augment:\r\n",
    "            self._transform_list.extend(\r\n",
    "                [\r\n",
    "                    A.VerticalFlip(p=0.5),\r\n",
    "                    A.HorizontalFlip(p=0.5),\r\n",
    "                    A.ShiftScaleRotate(\r\n",
    "                        scale_limit=0.20,\r\n",
    "                        rotate_limit=10,\r\n",
    "                        shift_limit=0.1,\r\n",
    "                        p=0.5,\r\n",
    "                        border_mode=cv2.BORDER_CONSTANT,\r\n",
    "                        value=0,\r\n",
    "                    ),\r\n",
    "                    A.RandomBrightnessContrast(p=0.5),\r\n",
    "                    # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\r\n",
    "                    # ToTensorV2(),\r\n",
    "                ]\r\n",
    "            )\r\n",
    "\r\n",
    "        if self._train or self._predict:\r\n",
    "            self._transform_list.extend(\r\n",
    "                [\r\n",
    "                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\r\n",
    "                    ToTensorV2(),\r\n",
    "                ]\r\n",
    "            )\r\n",
    "\r\n",
    "        if self._transform_list:\r\n",
    "\r\n",
    "            self._transforms = A.Compose(\r\n",
    "                self._transform_list,\r\n",
    "                bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"labels\"]),\r\n",
    "            )\r\n",
    "        self._preprocess_boxes()\r\n",
    "\r\n",
    "    def _preprocess_boxes(self):\r\n",
    "        self.id_bbox_map = {}\r\n",
    "        scaled_w = self._size[1]\r\n",
    "        scaled_h = self._size[0]\r\n",
    "        opacity_count = 0\r\n",
    "        none_count = 0\r\n",
    "        for i, id in enumerate(self.ids):\r\n",
    "            row = self.df.loc[id]\r\n",
    "            all_boxes = []\r\n",
    "            all_cats = []\r\n",
    "            if pd.notna(row[\"boxes\"]):\r\n",
    "                boxes = eval(row[\"boxes\"])\r\n",
    "                for box in boxes:\r\n",
    "                    # convert to center and normalize to 0, 1\r\n",
    "                    box[\"x\"] = (max(0, box[\"x\"])) * scaled_w / (row[\"width\"]) \r\n",
    "                    box[\"y\"] = (max(0, box[\"y\"])) * scaled_h / (row[\"height\"])\r\n",
    "                    box[\"width\"] = box[\"width\"] * scaled_w / row[\"width\"]\r\n",
    "                    box[\"height\"] = box[\"height\"] * scaled_h / row[\"height\"]\r\n",
    "                    bbox = [\r\n",
    "                        box[\"x\"],\r\n",
    "                        box[\"y\"],\r\n",
    "                        box[\"x\"] + (box[\"width\"]),\r\n",
    "                        box[\"y\"] + (box[\"height\"]),\r\n",
    "                    ]\r\n",
    "\r\n",
    "                    all_boxes.append(bbox)\r\n",
    "                    all_cats.append(self.label_list.index(\"opacity\") + 1)\r\n",
    "                    opacity_count += 1\r\n",
    "            else:\r\n",
    "                # setting the entire image as a negative \"detection\".\r\n",
    "                bbox = [0, 0, scaled_w, scaled_h]\r\n",
    "                all_boxes.append(bbox)\r\n",
    "                all_cats.append(self.label_list.index(\"none\") + 1)\r\n",
    "                none_count += 1\r\n",
    "            self.id_bbox_map[id] = (all_boxes, all_cats)\r\n",
    "        print(\"Opacity detections: {}\".format(opacity_count))\r\n",
    "        print(\"None Count: {}\".format(none_count))\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.ids)\r\n",
    "\r\n",
    "    def draw_bbox_idx(self, idx):\r\n",
    "        img_id = self.ids[idx]\r\n",
    "        row = self.df.loc[img_id]\r\n",
    "        print(img_id)\r\n",
    "        image = PIL.Image.open(row[\"path\"])\r\n",
    "        scaled_w = image.width\r\n",
    "        scaled_h = image.height\r\n",
    "        print((scaled_w, scaled_h))\r\n",
    "        if pd.notna(row[\"boxes\"]):\r\n",
    "            boxes = eval(row[\"boxes\"])\r\n",
    "            draw = PIL.ImageDraw.Draw(image)\r\n",
    "            for box in boxes:\r\n",
    "                box[\"x\"] = box[\"x\"] / row[\"width\"]\r\n",
    "                box[\"y\"] = box[\"y\"] / row[\"height\"]\r\n",
    "                box[\"width\"] = box[\"width\"] / row[\"width\"]\r\n",
    "                box[\"height\"] = box[\"height\"] / row[\"height\"]\r\n",
    "                draw.rectangle(\r\n",
    "                    [\r\n",
    "                        box[\"x\"] * scaled_w,\r\n",
    "                        box[\"y\"] * scaled_h,\r\n",
    "                        (box[\"x\"] + box[\"width\"]) * scaled_w,\r\n",
    "                        (box[\"y\"] + box[\"height\"]) * scaled_h,\r\n",
    "                    ]\r\n",
    "                )\r\n",
    "        return image\r\n",
    "\r\n",
    "    def _yolo_to_voc_format(self, yolo_bboxes):\r\n",
    "        # takes in bounding boxes of the yolo format\r\n",
    "        # converts them to voc format.\r\n",
    "        # yolo format (x_c, y_c, width, height) normalized to 0, 1 by dividing by image dims.\r\n",
    "        # voc format (x_min, y_min, x_max, y_max), unnormalized.\r\n",
    "        scaled_w = self._size[1]\r\n",
    "        scaled_h = self._size[0]\r\n",
    "        bboxes_voc = torch.zeros_like(yolo_bboxes)\r\n",
    "        # x_min = (x_c - width / 2) * scaled_w\r\n",
    "        bboxes_voc[:, 0] = (yolo_bboxes[:, 0] - yolo_bboxes[:, 2] / 2) * scaled_w\r\n",
    "        bboxes_voc[:, 1] = (yolo_bboxes[:, 1] - yolo_bboxes[:, 3] / 2) * scaled_h\r\n",
    "        bboxes_voc[:, 2] = bboxes_voc[:, 0] + yolo_bboxes[:, 2] * scaled_w\r\n",
    "        bboxes_voc[:, 3] = bboxes_voc[:, 1] + yolo_bboxes[:, 3] * scaled_h\r\n",
    "\r\n",
    "        return bboxes_voc\r\n",
    "\r\n",
    "    def draw_bbox_img(self, image, bboxes, label):\r\n",
    "        image = PIL.Image.fromarray(image)\r\n",
    "        draw = PIL.ImageDraw.Draw(image)\r\n",
    "        for bbox in bboxes:\r\n",
    "            # x_c = bbox[0]\r\n",
    "            # y_c = bbox[1]\r\n",
    "            # width = bbox[2]\r\n",
    "            # height = bbox[3]\r\n",
    "            # x_1 = (x_c - width / 2) * image.width\r\n",
    "            # y_1 = (y_c - height / 2) * image.height\r\n",
    "            # x_2 = x_1 + width * image.width\r\n",
    "            # y_2 = y_1 + height * image.height\r\n",
    "            # draw.rectangle([x_1, y_1, x_2, y_2])\r\n",
    "            draw.rectangle([bbox[0], bbox[1], bbox[2], bbox[3]])\r\n",
    "        print(f\"Number of boxes{len(label)}\")\r\n",
    "        return image\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        img_id = self.ids[idx]\r\n",
    "        boxes, labels = self.id_bbox_map[img_id]\r\n",
    "        row = self.df.loc[img_id]\r\n",
    "\r\n",
    "        path = row[\"path\"]\r\n",
    "        # ideally, we'd clean up the df,\r\n",
    "        # but may be we use it to produce predictions as well.\r\n",
    "        dicom_arr = (\r\n",
    "            cv2.imread(path)\r\n",
    "            if path.endswith(\".jpg\")\r\n",
    "            else dicom2array(path, size=self._size)\r\n",
    "        )\r\n",
    "        img = cv2.cvtColor(dicom_arr, cv2.COLOR_BGR2RGB)\r\n",
    "        image_and_labels = {}\r\n",
    "        if self._augment or (self._train or self._predict):\r\n",
    "            image_and_labels = self._transforms(image=img, bboxes=boxes, labels=labels)\r\n",
    "        else:\r\n",
    "            image_and_labels = {\"image\": img, \"bboxes\": boxes, \"labels\": labels}\r\n",
    "\r\n",
    "        # image_and_labels[\"bboxes\"] = self._yolo_to_voc_format(\r\n",
    "        #     torch.tensor(image_and_labels[\"bboxes\"])\r\n",
    "        # )\r\n",
    "        \r\n",
    "        if not image_and_labels[\"bboxes\"]:\r\n",
    "            w = torch.rand(1) * 10\r\n",
    "            h = torch.rand(1) * 10\r\n",
    "            image_and_labels[\"bboxes\"] = [[self._size[0] /2 - w, self._size[1]/2 - h, self._size[0]/2 + w, self._size[1]/2 + h]]\r\n",
    "            image_and_labels[\"labels\"] = [-1]\r\n",
    "        # print(image_and_labels[\"bboxes\"].shape)\r\n",
    "        image_and_labels[\"bboxes\"] = torch.tensor(image_and_labels[\"bboxes\"]).to(dtype=torch.float32)\r\n",
    "        image_and_labels[\"labels\"] = torch.tensor(image_and_labels[\"labels\"]).to(dtype=torch.long)\r\n",
    "\r\n",
    "        return image_and_labels\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_ds = XRayDatasetFromDFOpacityAnnotations(\r\n",
    "    train_samples_df, train=False, predict=False, augment=True, size=SIZE\r\n",
    ")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AverageMeter:\r\n",
    "    def __init__(self):\r\n",
    "        self.val = 0\r\n",
    "        self.avg = 0\r\n",
    "        self.sum = 0\r\n",
    "        self.count = 0\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.val = 0\r\n",
    "        self.avg = 0\r\n",
    "        self.sum = 0\r\n",
    "        self.count = 0\r\n",
    "\r\n",
    "    def update(self, val, n=1):\r\n",
    "        self.val = val\r\n",
    "        self.sum += val * n\r\n",
    "        self.count += n\r\n",
    "        self.avg = self.sum / self.count\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_epoch(\r\n",
    "    epoch,\r\n",
    "    step,\r\n",
    "    dataloader,\r\n",
    "    batchsize,\r\n",
    "    model_w_loss,\r\n",
    "    optimizer,\r\n",
    "    log_every=10,\r\n",
    "    scaler=None,\r\n",
    "):\r\n",
    "\r\n",
    "    steps = len(dataloader)\r\n",
    "    batchsize = batchsize\r\n",
    "    dataiter = iter(dataloader)\r\n",
    "\r\n",
    "    time_now = time.time()\r\n",
    "\r\n",
    "    total_loss_avg = AverageMeter()\r\n",
    "    cls_loss_avg = AverageMeter()\r\n",
    "    reg_loss_avg = AverageMeter()\r\n",
    "\r\n",
    "    model_w_loss.train()\r\n",
    "    loader = tqdm(range(steps))\r\n",
    "\r\n",
    "    enable_autocast = not (scaler == None)\r\n",
    "\r\n",
    "    for i in loader:\r\n",
    "        optimizer.zero_grad()\r\n",
    "        data = next(dataiter)\r\n",
    "        images, bboxes, labels = data[\"image\"], data[\"bboxes\"], data[\"labels\"]\r\n",
    "\r\n",
    "        images = images.to(dev)\r\n",
    "\r\n",
    "        bboxes = [bbox.float().to(dev) for bbox in bboxes]\r\n",
    "        labels = [label.to(dev) for label in labels]\r\n",
    "\r\n",
    "        with amp.autocast(enabled=enable_autocast):\r\n",
    "            # hacky timm target assembly\r\n",
    "            target = {\"bbox\": bboxes, \"cls\": labels}\r\n",
    "            losses = model_w_loss(images, target)\r\n",
    "            cls_loss, reg_loss = losses[\"class_loss\"], losses[\"box_loss\"]\r\n",
    "            # cls_loss = cls_loss.mean()\r\n",
    "            # reg_loss = reg_loss.mean()\r\n",
    "            loss = losses[\"loss\"]\r\n",
    "\r\n",
    "        if enable_autocast:\r\n",
    "            scaled_loss = scaler.scale(loss)\r\n",
    "            scaled_loss.backward()\r\n",
    "        else:\r\n",
    "            total_loss_avg.update(loss.item(), batchsize)\r\n",
    "            loss.backward()\r\n",
    "        total_loss_avg.update(loss.item(), batchsize)\r\n",
    "        cls_loss_avg.update(cls_loss.item(), batchsize)\r\n",
    "        reg_loss_avg.update(reg_loss.item(), batchsize)\r\n",
    "\r\n",
    "        if scaler:\r\n",
    "            scaler.step(optimizer)\r\n",
    "            scaler.update()\r\n",
    "        else:\r\n",
    "            optimizer.step()\r\n",
    "\r\n",
    "        time_spent = time.time() - time_now\r\n",
    "        time_now = time.time()\r\n",
    "\r\n",
    "        loader.set_description(\r\n",
    "            \"Training Epoch : {}, Time Spent {:.5g}, Step {}\".format(\r\n",
    "                epoch, time_spent, step\r\n",
    "            )\r\n",
    "        )\r\n",
    "        loader.set_postfix(\r\n",
    "            loss=total_loss_avg.avg,\r\n",
    "            reg_loss=reg_loss_avg.avg,\r\n",
    "            cls_loss=cls_loss_avg.avg,\r\n",
    "        )\r\n",
    "\r\n",
    "        step += 1\r\n",
    "\r\n",
    "    return cls_loss_avg.avg, reg_loss_avg.avg, total_loss_avg.avg, step\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def normalize_boxes_in_df(boxes_df):\r\n",
    "    boxes_df[\"XMin\"] = boxes_df[\"XMin\"] / SIZE[0]\r\n",
    "    boxes_df[\"YMin\"] = boxes_df[\"YMin\"] / SIZE[1]\r\n",
    "    boxes_df[\"XMax\"] = boxes_df[\"XMax\"] / SIZE[0]\r\n",
    "    boxes_df[\"YMax\"] = boxes_df[\"YMax\"] / SIZE[1]\r\n",
    "    return boxes_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def valid_epoch(epoch, step, dataloader, batchsize, model_w_loss, gt_df):\r\n",
    "    num_val = len(dataloader)\r\n",
    "\r\n",
    "    dataiter = iter(dataloader)\r\n",
    "    model_w_loss.eval()\r\n",
    "\r\n",
    "    detections = []\r\n",
    "\r\n",
    "    total_loss_avg = AverageMeter()\r\n",
    "    cls_loss_avg = AverageMeter()\r\n",
    "    reg_loss_avg = AverageMeter()\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for i in tqdm(range(num_val)):\r\n",
    "            data = next(dataiter)\r\n",
    "            images, bboxes, labels = data[\"image\"], data[\"bboxes\"], data[\"labels\"]\r\n",
    "            batchsize = images.shape[0]\r\n",
    "\r\n",
    "            images = images.to(dev)\r\n",
    "\r\n",
    "            bboxes = [bbox.float().to(dev) for bbox in bboxes]\r\n",
    "            labels = [label.to(dev) for label in labels]\r\n",
    "            target = {\r\n",
    "                \"bbox\": bboxes,\r\n",
    "                \"cls\": labels,\r\n",
    "                \"img_scale\": torch.tensor(\r\n",
    "                    [1.0] * images.shape[0], dtype=torch.float32\r\n",
    "                ).to(dev),\r\n",
    "                \"img_size\": torch.tensor(\r\n",
    "                    [images.shape[-2:]] * images.shape[0], dtype=torch.float32\r\n",
    "                ).to(dev),\r\n",
    "            }\r\n",
    "            losses_w_detections = model_w_loss(images, target)\r\n",
    "            cls_loss, reg_loss = (\r\n",
    "                losses_w_detections[\"class_loss\"],\r\n",
    "                losses_w_detections[\"box_loss\"],\r\n",
    "            )\r\n",
    "            loss = losses_w_detections[\"loss\"]\r\n",
    "            total_loss_avg.update(loss.item(), batchsize)\r\n",
    "            cls_loss_avg.update(cls_loss.item(), batchsize)\r\n",
    "            reg_loss_avg.update(reg_loss.item(), batchsize)\r\n",
    "\r\n",
    "            batched_detections = losses_w_detections[\"detections\"]\r\n",
    "            detections.append(batched_detections)\r\n",
    "\r\n",
    "        offset = 0\r\n",
    "        ids = dataloader.dataset.ids\r\n",
    "        label_list = dataloader.dataset.label_list\r\n",
    "        all_preds = []\r\n",
    "        for batched_detections in detections:\r\n",
    "            num_imgs, num_dets_per_img = (\r\n",
    "                batched_detections.shape[0],\r\n",
    "                batched_detections.shape[1],\r\n",
    "            )\r\n",
    "            img_ids = []\r\n",
    "\r\n",
    "            for i in range(num_imgs):\r\n",
    "                img_ids.extend([ids[offset]] * num_dets_per_img)\r\n",
    "                offset += 1\r\n",
    "\r\n",
    "            batched_preds_df = pd.DataFrame.from_records(\r\n",
    "                batched_detections.view(num_imgs * num_dets_per_img, -1).tolist(),\r\n",
    "                columns=[\"XMin\", \"YMin\", \"XMax\", \"YMax\", \"Conf\", \"LabelName\"],\r\n",
    "            )\r\n",
    "            batched_preds_df[\"ImageID\"] = img_ids\r\n",
    "            batched_preds_df[\"LabelName\"] = batched_preds_df.apply(\r\n",
    "                lambda x: label_list[int(x[\"LabelName\"] - 1)], axis=1\r\n",
    "            )\r\n",
    "            all_preds.append(batched_preds_df)\r\n",
    "        all_preds_df = pd.concat(all_preds)\r\n",
    "        print(all_preds_df.columns)\r\n",
    "        print(gt_df.columns)\r\n",
    "\r\n",
    "        # gt_df = normalize_boxes_in_df(gt_df)\r\n",
    "        # all_preds_df = normalize_boxes_in_df(all_preds_df)\r\n",
    "\r\n",
    "        mean_ap, average_precisions = mean_average_precision_for_boxes(\r\n",
    "            gt_df[[\"ImageID\", \"LabelName\", \"XMin\", \"XMax\", \"YMin\", \"YMax\"]],\r\n",
    "            all_preds_df[\r\n",
    "                [\"ImageID\", \"LabelName\", \"Conf\", \"XMin\", \"XMax\", \"YMin\", \"YMax\"]\r\n",
    "            ],\r\n",
    "            verbose=True,\r\n",
    "        )\r\n",
    "    # topk = (-probs).argsort(axis=1)[:, :2]\r\n",
    "    # # mapk = mean_average_precision(targets[:, np.newaxis].tolist(), topk.tolist(), k=2)\r\n",
    "    print(\"Mean AP @0.5 Score at Epoch {} and Step {}: {}\".format(epoch, step, mean_ap))\r\n",
    "    print(\r\n",
    "        \"AP @0.5 Score for Opacity at Epoch {} and Step {}: {}\".format(\r\n",
    "            epoch, step, average_precisions[\"opacity\"]\r\n",
    "        )\r\n",
    "    )\r\n",
    "    # print(\"AUC Score at Epoch {} and Step {}: {}\".format(epoch, step, auc))\r\n",
    "    return (\r\n",
    "        average_precisions[\"none\"][0],\r\n",
    "        total_loss_avg.avg,\r\n",
    "        cls_loss_avg.avg,\r\n",
    "        reg_loss_avg.avg,\r\n",
    "        average_precisions[\"opacity\"][0],\r\n",
    "        all_preds_df\r\n",
    "    )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_model(\r\n",
    "    model_w_loss,\r\n",
    "    epochs,\r\n",
    "    batchsize,\r\n",
    "    optimizer,\r\n",
    "    scheduler,\r\n",
    "    save_path,\r\n",
    "    train_dl,\r\n",
    "    validation_dl,\r\n",
    "    val_gt_df,\r\n",
    "    use_mp=True,\r\n",
    "):\r\n",
    "    if use_mp:\r\n",
    "        scaler = amp.GradScaler()\r\n",
    "    else:\r\n",
    "        scaler = None\r\n",
    "    if not os.path.exists(save_path):\r\n",
    "        os.mkdir(save_path)\r\n",
    "\r\n",
    "    train_vs_val = []\r\n",
    "\r\n",
    "    reg_loss_best = float(\"inf\")\r\n",
    "    mean_ap_best = -float(\"inf\")\r\n",
    "    ap_opacity_best = -float(\"inf\")\r\n",
    "    step = 0\r\n",
    "\r\n",
    "    for epoch in range(epochs):\r\n",
    "\r\n",
    "        train_cls_loss, train_reg_loss, train_total_loss, step = train_epoch(\r\n",
    "            epoch, step, train_dl, batchsize, model_w_loss, optimizer, scaler=scaler\r\n",
    "        )\r\n",
    "\r\n",
    "        time_now = time.time()\r\n",
    "        (\r\n",
    "            mean_ap,\r\n",
    "            val_total_loss,\r\n",
    "            val_cls_loss,\r\n",
    "            val_reg_loss,\r\n",
    "            ap_opacity,\r\n",
    "            preds_df,\r\n",
    "        ) = valid_epoch(epoch, step, validation_dl, batchsize, model_w_loss, val_gt_df)\r\n",
    "        # val_mapk, val_auc, val_loss_avg, probs, targets = valid_epoch(\r\n",
    "        #     epoch, step, validation_dl, batchsize * 2, model, loss_fn\r\n",
    "        # )\r\n",
    "\r\n",
    "        train_vs_val.extend(\r\n",
    "            [\r\n",
    "                (epoch, mean_ap, \"Mean AP\"),\r\n",
    "                (epoch, ap_opacity, \"Validation AP Opacity\"),\r\n",
    "                (epoch, val_total_loss, \"Validation Total Loss\"),\r\n",
    "                (epoch, val_reg_loss, \"Train Regression Loss\"),\r\n",
    "                (epoch, val_cls_loss, \"Train Classification Loss\"),\r\n",
    "                (epoch, train_total_loss, \"Training Total Loss\"),\r\n",
    "                (epoch, train_reg_loss, \"Train Regression Loss\"),\r\n",
    "                (epoch, train_cls_loss, \"Train Classification Loss\"),\r\n",
    "            ]\r\n",
    "        )\r\n",
    "\r\n",
    "        if scheduler:\r\n",
    "            scheduler.step(val_total_loss)\r\n",
    "            print(\r\n",
    "                \"Setting Learning Rate to: {:.6f}\".format(\r\n",
    "                    optimizer.param_groups[-1][\"lr\"]\r\n",
    "                )\r\n",
    "            )\r\n",
    "\r\n",
    "        if reg_loss_best > val_reg_loss:\r\n",
    "            print(\r\n",
    "                \"Found Model with best Regression Loss {} at epoch {}\".format(\r\n",
    "                    val_reg_loss, epoch\r\n",
    "                )\r\n",
    "            )\r\n",
    "            torch.save(\r\n",
    "                {\r\n",
    "                    \"epoch\": epoch,\r\n",
    "                    \"mAP\": mean_ap,\r\n",
    "                    \"TotalLoss\": val_total_loss,\r\n",
    "                    \"AP (Opacity)\": ap_opacity,\r\n",
    "                    \"state_dict\": model_w_loss.module.model.state_dict(),\r\n",
    "                },\r\n",
    "                os.path.join(save_path, \"best_reg_loss.pth\"),\r\n",
    "            )\r\n",
    "            preds_df.to_csv(\r\n",
    "                os.path.join(save_path, \"best_reg_loss_preds.csv\"), index=False\r\n",
    "            )\r\n",
    "            reg_loss_best = val_reg_loss\r\n",
    "\r\n",
    "        if reg_loss_best > val_total_loss:\r\n",
    "            print(\r\n",
    "                \"Found Model with best Total Loss {} at epoch {}\".format(\r\n",
    "                    val_total_loss, epoch\r\n",
    "                )\r\n",
    "            )\r\n",
    "            torch.save(\r\n",
    "                {\r\n",
    "                    \"epoch\": epoch,\r\n",
    "                    \"mAP\": mean_ap,\r\n",
    "                    \"TotalLoss\": val_total_loss,\r\n",
    "                    \"AP (Opacity)\": ap_opacity,\r\n",
    "                    \"state_dict\": model_w_loss.module.model.state_dict(),\r\n",
    "                },\r\n",
    "                os.path.join(save_path, \"best_total_loss.pth\"),\r\n",
    "            )\r\n",
    "            preds_df.to_csv(\r\n",
    "                os.path.join(save_path, \"best_total_loss_preds.csv\"), index=False\r\n",
    "            )\r\n",
    "            reg_loss_best = val_total_loss\r\n",
    "\r\n",
    "        if ap_opacity_best < ap_opacity:\r\n",
    "            print(\r\n",
    "                \"Found Model with best AP Opacity {} at epoch {}\".format(\r\n",
    "                    ap_opacity, epoch\r\n",
    "                )\r\n",
    "            )\r\n",
    "            torch.save(\r\n",
    "                {\r\n",
    "                    \"epoch\": epoch,\r\n",
    "                    \"mAP\": mean_ap,\r\n",
    "                    \"TotalLoss\": val_total_loss,\r\n",
    "                    \"AP (Opacity)\": ap_opacity,\r\n",
    "                    \"state_dict\": model_w_loss.module.model.state_dict(),\r\n",
    "                },\r\n",
    "                os.path.join(save_path, \"best_ap_opacity.pth\"),\r\n",
    "            )\r\n",
    "            ap_opacity_best = ap_opacity\r\n",
    "            preds_df.to_csv(\r\n",
    "                os.path.join(save_path, \"best_ap_opacity_preds.csv\"), index=False\r\n",
    "            )\r\n",
    "\r\n",
    "        if mean_ap_best < mean_ap:\r\n",
    "            print(\"Found Model with best Mean AP {} at epoch {}\".format(mean_ap, epoch))\r\n",
    "            torch.save(\r\n",
    "                {\r\n",
    "                    \"epoch\": epoch,\r\n",
    "                    \"mAP\": mean_ap,\r\n",
    "                    \"TotalLoss\": val_total_loss,\r\n",
    "                    \"AP (Opacity)\": ap_opacity,\r\n",
    "                    \"state_dict\": model_w_loss.module.model.state_dict(),\r\n",
    "                },\r\n",
    "                os.path.join(save_path, \"best_mean_ap.pth\"),\r\n",
    "            )\r\n",
    "            mean_ap_best = mean_ap\r\n",
    "            preds_df.to_csv(\r\n",
    "                os.path.join(save_path, \"best_mean_ap_preds.csv\"), index=False\r\n",
    "            )\r\n",
    "\r\n",
    "        time_spent = time.time() - time_now\r\n",
    "        print(\r\n",
    "            \"{}, Epoch : {}, Step : {}, Validation Loss : {:.5f}, Mean AP : {:.5f}, AP Opacity : {:.5f}, Run Time : {:.5g}\".format(\r\n",
    "                time.strftime(\"%Y-%m-%d %H:%M:%S\"),\r\n",
    "                epoch,\r\n",
    "                step,\r\n",
    "                val_total_loss,\r\n",
    "                mean_ap,\r\n",
    "                ap_opacity,\r\n",
    "                time_spent,\r\n",
    "            )\r\n",
    "        )\r\n",
    "\r\n",
    "    return train_vs_val\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def collater(data):\r\n",
    "    imgs = [s[\"image\"] for s in data]\r\n",
    "    bboxes = []\r\n",
    "    labels = []\r\n",
    "    for i, s in enumerate(data):\r\n",
    "        # if len(s[\"bboxes\"].shape) != 2:\r\n",
    "        #     print(\"came here\")\r\n",
    "        #     width = torch.rand(1) * 10\r\n",
    "        #     height = torch.rand(1) * 10\r\n",
    "        #     bboxes.append(torch.rand((4)) * 10)\r\n",
    "        #     labels.append(torch.tensor([[1]]))\r\n",
    "        # else:\r\n",
    "            # rearrange to timm format\r\n",
    "        bboxes.append(s[\"bboxes\"][:, [1, 0, 3, 2]])\r\n",
    "        labels.append(s[\"labels\"])\r\n",
    "    # labels = [s[\"labels\"] for s in data]\r\n",
    "\r\n",
    "    imgs = torch.stack(imgs, dim=0)\r\n",
    "\r\n",
    "    max_boxes = max(bbox.shape[0] for bbox in bboxes)\r\n",
    "\r\n",
    "    # if max_boxes > 0:\r\n",
    "\r\n",
    "    #     annot_padded = torch.ones((len(bboxes), max_boxes, 4)) * -1\r\n",
    "\r\n",
    "    #     for idx, annot in enumerate(bboxes):\r\n",
    "    #         if annot.shape[0] > 0:\r\n",
    "    #             annot_padded[idx, :annot.shape[0], :-1] = annot\r\n",
    "    #             annot_padded[idx, :annot.shape[0], -1] = labels[idx]\r\n",
    "    # else:\r\n",
    "    #     annot_padded = torch.ones((len(bboxes), 1, 5)) * -1\r\n",
    "\r\n",
    "    # imgs = imgs.permute(0, 3, 1, 2)\r\n",
    "\r\n",
    "    return {\"image\": imgs, \"bboxes\": bboxes, \"labels\": labels}\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_gt_df_from_ds(dataset):\r\n",
    "    label_list = dataset.label_list\r\n",
    "    gt_df = []\r\n",
    "    for i, data in enumerate(dataset):\r\n",
    "        bboxes = data[\"bboxes\"].tolist()\r\n",
    "        labels = data[\"labels\"].tolist()\r\n",
    "        id = dataset.ids[i]\r\n",
    "        for j, box in enumerate(bboxes):\r\n",
    "            box.extend([labels[j], id])\r\n",
    "        \r\n",
    "        img_df = pd.DataFrame.from_records(\r\n",
    "            bboxes, columns=[\"XMin\", \"YMin\", \"XMax\", \"YMax\", \"LabelName\", \"ImageID\"]\r\n",
    "        )\r\n",
    "        gt_df.append(img_df)\r\n",
    "    print(pd.concat(gt_df).columns)\r\n",
    "    gt_df = pd.concat(gt_df)\r\n",
    "    gt_df[\"LabelName\"] = gt_df.apply(lambda x: label_list[int(x[\"LabelName\"] - 1)], axis=1)\r\n",
    "    return gt_df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_net():\r\n",
    "    config = get_efficientdet_config(MODEL_NAME)\r\n",
    "\r\n",
    "    config.image_size = SIZE\r\n",
    "    config.norm_kwargs = dict(eps=0.001, momentum=0.01)\r\n",
    "\r\n",
    "    net = EfficientDet(config, pretrained_backbone=True)\r\n",
    "    net.reset_head(num_classes=2)\r\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\r\n",
    "    print(config)\r\n",
    "\r\n",
    "    return DetBenchTrain(net, config)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for fold in range(FOLDS):\r\n",
    "    print(\"Training Fold {}\".format(fold))\r\n",
    "    model = get_net()\r\n",
    "\r\n",
    "    model = torch.nn.DataParallel(model)\r\n",
    "    model = model.to(dev)\r\n",
    "\r\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\r\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n",
    "        optimizer, mode=\"min\", factor=0.1, patience=5, verbose=False, min_lr=1e-7\r\n",
    "    )\r\n",
    "    \r\n",
    "    training_fold = train_samples_df[train_samples_df[\"fold\"] != fold]\r\n",
    "    validation_fold = train_samples_df[train_samples_df[\"fold\"] == fold]\r\n",
    "    \r\n",
    "    validation_fold.to_csv(\"validation_fold-{}\".format(fold))\r\n",
    "    training_fold.to_csv(\"training_fold-{}\".format(fold))\r\n",
    "    training_ds = XRayDatasetFromDFOpacityAnnotations(\r\n",
    "        train_samples_df[train_samples_df[\"fold\"] != fold],\r\n",
    "        augment=True,\r\n",
    "        train=True,\r\n",
    "        size=SIZE,\r\n",
    "    )\r\n",
    "    validation_ds = XRayDatasetFromDFOpacityAnnotations(\r\n",
    "        train_samples_df[train_samples_df[\"fold\"] == fold],\r\n",
    "        predict=True,\r\n",
    "        augment=False,\r\n",
    "        size=SIZE,\r\n",
    "    )\r\n",
    "\r\n",
    "    val_df = make_gt_df_from_ds(validation_ds)\r\n",
    "    print(\"{} Ground Truth size\".format(str(val_df.shape)))\r\n",
    "    print(\"{} train len {} val len\".format(len(training_ds), len(validation_ds)))\r\n",
    "\r\n",
    "    training_dl = torch.utils.data.DataLoader(\r\n",
    "        dataset=training_ds,\r\n",
    "        batch_size=BATCHSIZE,\r\n",
    "        pin_memory=True,\r\n",
    "        num_workers=0,\r\n",
    "        drop_last=False,\r\n",
    "        shuffle=True,\r\n",
    "        collate_fn=collater,\r\n",
    "        # prefetch_factor=8,\r\n",
    "    )\r\n",
    "    validation_dl = torch.utils.data.DataLoader(\r\n",
    "        dataset=validation_ds,\r\n",
    "        batch_size=BATCHSIZE,\r\n",
    "        pin_memory=True,\r\n",
    "        num_workers=0,\r\n",
    "        shuffle=False,\r\n",
    "        drop_last=False,\r\n",
    "        # prefetch_factor=8,\r\n",
    "        collate_fn=collater,\r\n",
    "    )\r\n",
    "\r\n",
    "    print(\r\n",
    "        \"{} training data loader size {} validation dataloader size\".format(\r\n",
    "            len(training_dl), len(validation_dl)\r\n",
    "        )\r\n",
    "    )\r\n",
    "\r\n",
    "    train_vs_val = train_model(\r\n",
    "        model_w_loss=model,\r\n",
    "        epochs=20,\r\n",
    "        optimizer=optimizer,\r\n",
    "        scheduler=scheduler,\r\n",
    "        batchsize=BATCHSIZE,\r\n",
    "        save_path=\"{}-{}\".format(MODEL_NAME, fold),\r\n",
    "        train_dl=training_dl,\r\n",
    "        validation_dl=validation_dl,\r\n",
    "        val_gt_df=val_df,\r\n",
    "    )\r\n",
    "\r\n",
    "    fold_report = pd.DataFrame.from_records(\r\n",
    "        data=train_vs_val, columns=[\"Epoch\", \"Loss\", \"Type\"]\r\n",
    "    )\r\n",
    "    fold_report.to_csv(\"fold-{}-report.csv\".format(fold), index=False)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28dd899faa11f55be1317e8a7ce0944a390660f5df9cb7db7fa985085f3b108b"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}