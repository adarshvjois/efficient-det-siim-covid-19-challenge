{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import glob\r\n",
    "import os\r\n",
    "import time\r\n",
    "import random\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import PIL\r\n",
    "\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "\r\n",
    "import cv2\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "from tqdm.contrib.concurrent import process_map\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch.utils.data.dataset import Dataset\r\n",
    "import torch.cuda.amp as amp\r\n",
    "import pydicom\r\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\r\n",
    "\r\n",
    "import albumentations as A\r\n",
    "from albumentations.pytorch import ToTensorV2\r\n",
    "\r\n",
    "from map_boxes import mean_average_precision_for_boxes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATA_DIR = \"../\"\r\n",
    "RESIZE_DIR = \"../\"\r\n",
    "\r\n",
    "SIZE = (512, 512)\r\n",
    "FOLDS = 5\r\n",
    "NUM_CLASSES = 4\r\n",
    "BATCHSIZE = 8\r\n",
    "SEED = 420\r\n",
    "MODEL_NAME = \"tf_efficientdet_d0\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class XRayDatasetFromDFOpacityAnnotations(Dataset):\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        df,\r\n",
    "        train=True,\r\n",
    "        predict=True,\r\n",
    "        augment=True,\r\n",
    "        data_dir=os.path.join(DATA_DIR, \"train\"),\r\n",
    "        size=(384, 384),\r\n",
    "    ):\r\n",
    "        self.df = df\r\n",
    "        self.label_list = [\"opacity\"]\r\n",
    "        self.ids = df.index.sort_values()#[:100]\r\n",
    "        self.path_suffix = data_dir\r\n",
    "        self._augment = augment\r\n",
    "        self._train = train\r\n",
    "        self._predict = predict\r\n",
    "        self._size = size\r\n",
    "        self._transform_list = [\r\n",
    "            # A.Resize(size[0], size[1], p=1)\r\n",
    "        ]\r\n",
    "\r\n",
    "        if self._augment:\r\n",
    "            self._transform_list.extend(\r\n",
    "                [\r\n",
    "#                     A.VerticalFlip(p=0.5),\r\n",
    "#                     A.HorizontalFlip(p=0.5),\r\n",
    "#                     A.ShiftScaleRotate(\r\n",
    "#                         scale_limit=0.20,\r\n",
    "#                         rotate_limit=10,\r\n",
    "#                         shift_limit=0.1,\r\n",
    "#                         p=0.5,\r\n",
    "#                         border_mode=cv2.BORDER_CONSTANT,\r\n",
    "#                         value=0,\r\n",
    "#                     ),\r\n",
    "                    A.RandomBrightnessContrast(p=0.5),\r\n",
    "                    # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\r\n",
    "                    # ToTensorV2(),\r\n",
    "                ]\r\n",
    "            )\r\n",
    "\r\n",
    "        if self._train or self._predict:\r\n",
    "            self._transform_list.extend(\r\n",
    "                [\r\n",
    "                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\r\n",
    "                    ToTensorV2(),\r\n",
    "                ]\r\n",
    "            )\r\n",
    "\r\n",
    "        if self._transform_list:\r\n",
    "\r\n",
    "            self._transforms = A.Compose(\r\n",
    "                self._transform_list,\r\n",
    "                bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"labels\"]),\r\n",
    "            )\r\n",
    "        self._preprocess_boxes()\r\n",
    "\r\n",
    "    def _preprocess_boxes(self):\r\n",
    "        self.id_bbox_map = {}\r\n",
    "        scaled_w = self._size[1]\r\n",
    "        scaled_h = self._size[0]\r\n",
    "        opacity_count = 0\r\n",
    "        none_count = 0\r\n",
    "        for i, id in enumerate(self.ids):\r\n",
    "            row = self.df.loc[id]\r\n",
    "            all_boxes = []\r\n",
    "            all_cats = []\r\n",
    "            if pd.notna(row[\"boxes\"]):\r\n",
    "                boxes = eval(row[\"boxes\"])\r\n",
    "                for box in boxes:\r\n",
    "                    # convert to center and normalize to 0, 1\r\n",
    "                    box[\"x\"] = (max(0, box[\"x\"])) * scaled_w / (row[\"width\"]) \r\n",
    "                    box[\"y\"] = (max(0, box[\"y\"])) * scaled_h / (row[\"height\"])\r\n",
    "                    box[\"width\"] = box[\"width\"] * scaled_w / row[\"width\"]\r\n",
    "                    box[\"height\"] = box[\"height\"] * scaled_h / row[\"height\"]\r\n",
    "                    bbox = [\r\n",
    "                        box[\"x\"],\r\n",
    "                        box[\"y\"],\r\n",
    "                        box[\"x\"] + (box[\"width\"]),\r\n",
    "                        box[\"y\"] + (box[\"height\"]),\r\n",
    "                    ]\r\n",
    "\r\n",
    "                    all_boxes.append(bbox)\r\n",
    "                    all_cats.append(self.label_list.index(\"opacity\") + 1)\r\n",
    "                    opacity_count += 1\r\n",
    "            else:\r\n",
    "                # setting the entire image as a negative \"detection\".\r\n",
    "                bbox = [0, 0, scaled_w, scaled_h]\r\n",
    "                all_boxes.append(bbox)\r\n",
    "                all_cats.append(self.label_list.index(\"none\") + 1)\r\n",
    "                none_count += 1\r\n",
    "            self.id_bbox_map[id] = (all_boxes, all_cats)\r\n",
    "        print(\"Opacity detections: {}\".format(opacity_count))\r\n",
    "        print(\"None Count: {}\".format(none_count))\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.ids)\r\n",
    "\r\n",
    "    def draw_bbox_idx(self, idx):\r\n",
    "        img_id = self.ids[idx]\r\n",
    "        row = self.df.loc[img_id]\r\n",
    "        print(img_id)\r\n",
    "        image = PIL.Image.open(row[\"path\"])\r\n",
    "        scaled_w = image.width\r\n",
    "        scaled_h = image.height\r\n",
    "        print((scaled_w, scaled_h))\r\n",
    "        if pd.notna(row[\"boxes\"]):\r\n",
    "            boxes = eval(row[\"boxes\"])\r\n",
    "            draw = PIL.ImageDraw.Draw(image)\r\n",
    "            for box in boxes:\r\n",
    "                box[\"x\"] = box[\"x\"] / row[\"width\"]\r\n",
    "                box[\"y\"] = box[\"y\"] / row[\"height\"]\r\n",
    "                box[\"width\"] = box[\"width\"] / row[\"width\"]\r\n",
    "                box[\"height\"] = box[\"height\"] / row[\"height\"]\r\n",
    "                draw.rectangle(\r\n",
    "                    [\r\n",
    "                        box[\"x\"] * scaled_w,\r\n",
    "                        box[\"y\"] * scaled_h,\r\n",
    "                        (box[\"x\"] + box[\"width\"]) * scaled_w,\r\n",
    "                        (box[\"y\"] + box[\"height\"]) * scaled_h,\r\n",
    "                    ]\r\n",
    "                )\r\n",
    "        return image\r\n",
    "\r\n",
    "    def _yolo_to_voc_format(self, yolo_bboxes):\r\n",
    "        # takes in bounding boxes of the yolo format\r\n",
    "        # converts them to voc format.\r\n",
    "        # yolo format (x_c, y_c, width, height) normalized to 0, 1 by dividing by image dims.\r\n",
    "        # voc format (x_min, y_min, x_max, y_max), unnormalized.\r\n",
    "        scaled_w = self._size[1]\r\n",
    "        scaled_h = self._size[0]\r\n",
    "        bboxes_voc = torch.zeros_like(yolo_bboxes)\r\n",
    "        # x_min = (x_c - width / 2) * scaled_w\r\n",
    "        bboxes_voc[:, 0] = (yolo_bboxes[:, 0] - yolo_bboxes[:, 2] / 2) * scaled_w\r\n",
    "        bboxes_voc[:, 1] = (yolo_bboxes[:, 1] - yolo_bboxes[:, 3] / 2) * scaled_h\r\n",
    "        bboxes_voc[:, 2] = bboxes_voc[:, 0] + yolo_bboxes[:, 2] * scaled_w\r\n",
    "        bboxes_voc[:, 3] = bboxes_voc[:, 1] + yolo_bboxes[:, 3] * scaled_h\r\n",
    "\r\n",
    "        return bboxes_voc\r\n",
    "\r\n",
    "    def draw_bbox_img(self, image, bboxes, label):\r\n",
    "        image = PIL.Image.fromarray(image)\r\n",
    "        draw = PIL.ImageDraw.Draw(image)\r\n",
    "        for bbox in bboxes:\r\n",
    "            # x_c = bbox[0]\r\n",
    "            # y_c = bbox[1]\r\n",
    "            # width = bbox[2]\r\n",
    "            # height = bbox[3]\r\n",
    "            # x_1 = (x_c - width / 2) * image.width\r\n",
    "            # y_1 = (y_c - height / 2) * image.height\r\n",
    "            # x_2 = x_1 + width * image.width\r\n",
    "            # y_2 = y_1 + height * image.height\r\n",
    "            # draw.rectangle([x_1, y_1, x_2, y_2])\r\n",
    "            draw.rectangle([bbox[0], bbox[1], bbox[2], bbox[3]])\r\n",
    "        print(f\"Number of boxes{len(label)}\")\r\n",
    "        return image\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        img_id = self.ids[idx]\r\n",
    "        boxes, labels = self.id_bbox_map[img_id]\r\n",
    "        row = self.df.loc[img_id]\r\n",
    "\r\n",
    "        path = row[\"path\"]\r\n",
    "        # ideally, we'd clean up the df,\r\n",
    "        # but may be we use it to produce predictions as well.\r\n",
    "        dicom_arr = (\r\n",
    "            cv2.imread(path)\r\n",
    "            if path.endswith(\".jpg\")\r\n",
    "            else dicom2array(path, size=self._size)\r\n",
    "        )\r\n",
    "        img = cv2.cvtColor(dicom_arr, cv2.COLOR_BGR2RGB)\r\n",
    "        image_and_labels = {}\r\n",
    "        if self._augment or (self._train or self._predict):\r\n",
    "            image_and_labels = self._transforms(image=img, bboxes=boxes, labels=labels)\r\n",
    "        else:\r\n",
    "            image_and_labels = {\"image\": img, \"bboxes\": boxes, \"labels\": labels}\r\n",
    "\r\n",
    "        # image_and_labels[\"bboxes\"] = self._yolo_to_voc_format(\r\n",
    "        #     torch.tensor(image_and_labels[\"bboxes\"])\r\n",
    "        # )\r\n",
    "        \r\n",
    "        if not image_and_labels[\"bboxes\"]:\r\n",
    "            w = torch.rand(1) * 10\r\n",
    "            h = torch.rand(1) * 10\r\n",
    "            image_and_labels[\"bboxes\"] = [[self._size[0] /2 - w, self._size[1]/2 - h, self._size[0]/2 + w, self._size[1]/2 + h]]\r\n",
    "            image_and_labels[\"labels\"] = [-1]\r\n",
    "        # print(image_and_labels[\"bboxes\"].shape)\r\n",
    "        image_and_labels[\"bboxes\"] = torch.tensor(image_and_labels[\"bboxes\"]).to(dtype=torch.float32)\r\n",
    "        image_and_labels[\"labels\"] = torch.tensor(image_and_labels[\"labels\"]).to(dtype=torch.long)\r\n",
    "\r\n",
    "        return image_and_labels\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission_df = pd.read_csv('ImagePredictionsFormatted.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(submission_df.imageid.unique())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(submission_df[submission_df[\"conf\"] >= 0.2].imageid.unique())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission_df[submission_df[\"conf\"] >= 0.2].columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.kdeplot(data=submission_df[submission_df[\"conf\"] >= 0.2], x=\"conf\", hue=\"label\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "validation_df = pd.read_csv('C:/Users/adars/Workspaces/covid19-contest-working-dir/efficient-det/validation_fold-4')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "validation_ds = XRayDatasetFromDFOpacityAnnotations(df=validation_df, train=False, predict=False, augment=True, size=SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx = 101\r\n",
    "img_id = os.path.splitext(os.path.basename(validation_df.loc[idx][\"path\"]))[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "validation_ds.draw_bbox_idx(idx)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = validation_ds[idx]\r\n",
    "image = data[\"image\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[\"bboxes\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preds_df = pd.read_csv('C:/Users/adars/Workspaces/covid19-contest-working-dir/efficient-det/best_mean_ap_preds.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preds_df[preds_df[\"ImageID\"].str.contains(img_id)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "boxes = preds_df[(preds_df[\"ImageID\"].str.contains(img_id)) & (preds_df[\"Conf\"] >= 0.35)][[\"XMin\", \"YMin\", \"XMax\", \"YMax\"]].values\r\n",
    "labels = preds_df[(preds_df[\"ImageID\"].str.contains(img_id)) & (preds_df[\"Conf\"] >= 0.35)][[\"LabelName\"]].values\r\n",
    "confs = preds_df[(preds_df[\"ImageID\"].str.contains(img_id)) & (preds_df[\"Conf\"] >= 0.35)][[\"Conf\"]].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "confs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "validation_ds.draw_bbox_img(image, boxes, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "28dd899faa11f55be1317e8a7ce0944a390660f5df9cb7db7fa985085f3b108b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}